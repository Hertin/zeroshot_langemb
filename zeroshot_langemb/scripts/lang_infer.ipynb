{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prompt-producer",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daily-favor",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "from espnet.asr.pytorch_backend.asr import load_trained_model\n",
    "if 'cd' not in globals():\n",
    "    cd = True\n",
    "    os.chdir('..')\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sustained-atmosphere",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:reading model parameters from exp/train_pytorch_wav2vecfex/results/snapshot.ep.30\n",
      "WARNING:root:idim 512\n",
      "WARNING:root:fix feature extractor\n",
      "WARNING:root:warpctc_length_average False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "i = 30\n",
    "model, train_args = load_trained_model(f'exp/train_pytorch_wav2vecfex/results/snapshot.ep.{i}')\n",
    "device = torch.device('cuda')\n",
    "model = model.float()\n",
    "model = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "framed-spring",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "humanitarian-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dump/train/deltafalse/data.json.npy') as f:\n",
    "    js = json.load(f)['utts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abstract-commission",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lang(d):\n",
    "    s = d.split('_')[0]\n",
    "    s = re.sub(r'\\d+$', '', s.split('-')[0]) if re.search('[a-zA-Z]+', s) else s\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-naples",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "corrected-fellowship",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "embedded-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 1000\n",
    "frame_ratio = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cleared-double",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35839])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "affiliated-lawyer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4414e-04, -1.8311e-04,  2.4414e-04,  ...,  1.8311e-04,\n",
       "         -6.1035e-05, -1.5259e-04]], device='cuda:0')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.as_tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ambient-watch",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743ecd1453ce46fbb19f5be608ab7de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "random.seed(1)\n",
    "with torch.no_grad():\n",
    "    for k, v in tqdm(random.sample(js.items(), n_sample)):\n",
    "        lang = get_lang(k)\n",
    "#         x = torch.FloatTensor(np.load(v['input'][0]['feat'])).unsqueeze(0).to(device)\n",
    "        x = torch.FloatTensor(np.load(v['input'][0]['feat'])).to(device)\n",
    "        features = model.encode(x)\n",
    "        \n",
    "        features = features.squeeze(0).detach().cpu().numpy()\n",
    "        T, d = features.shape\n",
    "        n_frame = int(frame_ratio * T)\n",
    "        ys.extend([lang] * n_frame)\n",
    "        idx = random.sample(list(range(T)), n_frame)\n",
    "        xs.append(features[idx])\n",
    "#         break\n",
    "    xs = np.vstack(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "described-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(ys):\n",
    "    uniq = sorted(list(np.unique(ys)))\n",
    "    \n",
    "    out = np.zeros((len(ys), len(uniq)))\n",
    "    l2int = {l: i for i, l in enumerate(uniq)}\n",
    "    print(l2int)\n",
    "    for i, l in enumerate(ys):\n",
    "        out[i, l2int[l]] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "mysterious-hollow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['103',\n",
       " '107',\n",
       " '206',\n",
       " '307',\n",
       " '402',\n",
       " '404',\n",
       " 'BG',\n",
       " 'CH',\n",
       " 'CZ',\n",
       " 'FR',\n",
       " 'GE',\n",
       " 'N',\n",
       " 'PO',\n",
       " 'TH',\n",
       " 'TU']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq = sorted(list(np.unique(ys)))\n",
    "uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "potential-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_int(ys):\n",
    "    uniq = sorted(list(np.unique(ys)))\n",
    "    \n",
    "#     out = np.zeros((len(ys), len(uniq)))\n",
    "    l2int = {l: i for i, l in enumerate(uniq)}\n",
    "    print(l2int)\n",
    "    out = np.array([l2int[l] for i, l in enumerate(ys)])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "generic-contact",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'103': 0, '107': 1, '206': 2, '307': 3, '402': 4, '404': 5, 'BG': 6, 'CH': 7, 'CZ': 8, 'FR': 9, 'GE': 10, 'N': 11, 'PO': 12, 'TH': 13, 'TU': 14}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((22658, 256), (22658,))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(xs)\n",
    "Y = to_int(ys)\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ethical-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "selected-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "spatial-pakistan",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hertin/Software/miniconda3/envs/espnet-lowres/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC(random_state=0, max_iter=5000).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "revolutionary-athletics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "sophisticated-robert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9880847308031774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hertin/Software/miniconda3/envs/espnet-lowres/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# encode\n",
    "clf = LinearSVC(random_state=0).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "collected-prefix",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "turned-penalty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.9880847308031774 f1_macro 0.9910002299893931 f1_micro 0.9880847308031774\n",
      "[('103', 0.9811827956989247),\n",
      " ('107', 0.9846827133479211),\n",
      " ('206', 0.9875),\n",
      " ('307', 0.9857954545454546),\n",
      " ('402', 0.9679144385026738),\n",
      " ('404', 0.9837133550488599),\n",
      " ('BG', 1.0),\n",
      " ('CH', 0.9931972789115646),\n",
      " ('CZ', 1.0),\n",
      " ('FR', 0.9957805907172996),\n",
      " ('GE', 1.0),\n",
      " ('N', 0.9986320109439125),\n",
      " ('PO', 0.9936305732484076),\n",
      " ('TH', 0.9929742388758782),\n",
      " ('TU', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "uniq = sorted(list(np.unique(ys)))\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "f1 = f1_score(y_test, y_pred, average=None)\n",
    "\n",
    "print('acc', acc, 'f1_macro', f1_macro, 'f1_micro', f1_micro)\n",
    "pprint(list(zip(uniq, f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extractor\n",
    "acc = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-delta",
   "metadata": {},
   "source": [
    "base 0.9880847308031774 lgcn 0.9995374653098983 lemb 0.9993061979648473\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-aurora",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgcn+lagn 0.5062442183163737 lemb+lagn 0.7414431082331174"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-frank",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-secondary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgcn 0\n",
    "acc 0.5062442183163737 f1_macro 0.462408857271572 f1_micro 0.5062442183163737\n",
    "[('103', 0.4207920792079208),\n",
    " ('107', 0.6154847104749512),\n",
    " ('206', 0.46926229508196726),\n",
    " ('307', 0.3880597014925374),\n",
    " ('402', 0.32902033271719033),\n",
    " ('404', 0.5421558164354322),\n",
    " ('BG', 0.3891891891891892),\n",
    " ('CH', 0.5130434782608695),\n",
    " ('CZ', 0.3595505617977528),\n",
    " ('FR', 0.457286432160804),\n",
    " ('GE', 0.32335329341317365),\n",
    " ('N', 0.6835187057633972),\n",
    " ('PO', 0.32558139534883723),\n",
    " ('TH', 0.6108374384236454),\n",
    " ('TU', 0.5089974293059126)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgcn N\n",
    "acc 0.5238205365402405 f1_macro 0.4922570593656955 f1_micro 0.5238205365402405\n",
    "[('103', 0.38944723618090454),\n",
    " ('107', 0.5768976897689769),\n",
    " ('206', 0.44691607684529827),\n",
    " ('307', 0.3955696202531645),\n",
    " ('402', 0.3404255319148936),\n",
    " ('404', 0.5520361990950224),\n",
    " ('BG', 0.44198895027624313),\n",
    " ('CH', 0.496),\n",
    " ('CZ', 0.40714285714285714),\n",
    " ('FR', 0.5626373626373626),\n",
    " ('GE', 0.4023668639053254),\n",
    " ('N', 0.8602620087336244),\n",
    " ('PO', 0.3157894736842105),\n",
    " ('TH', 0.6278481012658228),\n",
    " ('TU', 0.5685279187817259)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "                         | lemb\n",
    "audio -> featureextractor -> encoder -> ctc\n",
    "                         |-> 46 %    |\n",
    "                                     |-> 98 / 99\n",
    "                                     |-> 30 - 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemb 0\n",
    "[('103', 0.5192307692307692),\n",
    " ('107', 0.8364153627311522),\n",
    " ('206', 0.6685823754789272),\n",
    " ('307', 0.6390685640362225),\n",
    " ('402', 0.5368248772504092),\n",
    " ('404', 0.7046263345195729),\n",
    " ('BG', 0.9463414634146341),\n",
    " ('CH', 0.9008264462809917),\n",
    " ('CZ', 0.8553459119496856),\n",
    " ('FR', 0.7598039215686274),\n",
    " ('GE', 0.736842105263158),\n",
    " ('N', 0.9328782707622297),\n",
    " ('PO', 0.5439330543933055),\n",
    " ('TH', 0.8956043956043955),\n",
    " ('TU', 0.8443271767810027)]\n",
    "\n",
    "penalize the language clf loss\n",
    "language model training data\n",
    "should phoneme boundary change depend on a/i/ei, \n",
    "replacing the phoible phoneme with langnet phoneme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lemb N\n",
    "acc 0.6667437557816837 f1_macro 0.6393575378750671 f1_micro 0.6667437557816837\n",
    "[('103', 0.5204359673024523),\n",
    " ('107', 0.7983134223471539),\n",
    " ('206', 0.5985985985985987),\n",
    " ('307', 0.5620542082738944),\n",
    " ('402', 0.504950495049505),\n",
    " ('404', 0.6666666666666667),\n",
    " ('BG', 0.8095238095238095),\n",
    " ('CH', 0.6419753086419754),\n",
    " ('CZ', 0.5752508361204013),\n",
    " ('FR', 0.6057692307692307),\n",
    " ('GE', 0.5227272727272727),\n",
    " ('N', 0.9234184239733628),\n",
    " ('PO', 0.4232365145228215),\n",
    " ('TH', 0.7537688442211056),\n",
    " ('TU', 0.6836734693877552)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "french-stream",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1747148139246091, 0.4387434699442588)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "lemblinear: w2v + linear\n",
    "acc 0.45050878815911194 f1_macro 0.4387434699442588 f1_micro 0.450508788159112\n",
    "'''\n",
    "f1_dict = OrderedDict([('103', 0.2857142857142857),\n",
    " ('107', 0.4832451499118166),\n",
    " ('206', 0.342451874366768),\n",
    " ('307', 0.2576489533011272),\n",
    " ('402', 0.167420814479638),\n",
    " ('404', 0.4682203389830508),\n",
    " ('BG', 0.7419354838709677),\n",
    " ('CH', 0.2119205298013245),\n",
    " ('CZ', 0.4892086330935252),\n",
    " ('FR', 0.3937007874015748),\n",
    " ('GE', 0.5391304347826087),\n",
    " ('N', 0.6977973568281938),\n",
    " ('PO', 0.31527093596059114),\n",
    " ('TH', 0.7109974424552431),\n",
    " ('TU', 0.47648902821316613)])\n",
    "np.std(list(f1_dict.values())), np.mean(list(f1_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-assets",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc 0.7717391304347826 f1_macro 0.7520866120652767 f1_micro 0.7717391304347826\n",
    "[('103', 0.726161369193154),\n",
    " ('107', 0.864748201438849),\n",
    " ('206', 0.7109375000000001),\n",
    " ('307', 0.7085714285714286),\n",
    " ('402', 0.5017182130584192),\n",
    " ('404', 0.8312849162011173),\n",
    " ('BG', 0.71875),\n",
    " ('CH', 0.7983193277310925),\n",
    " ('CZ', 0.6529209621993127),\n",
    " ('FR', 0.7476635514018691),\n",
    " ('GE', 0.7177033492822967),\n",
    " ('N', 0.9622857142857143),\n",
    " ('PO', 0.7383512544802867),\n",
    " ('TH', 0.8268156424581005),\n",
    " ('TU', 0.7750677506775068)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-redhead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "black-singing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "legislative-warrant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.17313864857382286, 0.4340926304476996)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "lemblinear: w2v only\n",
    "acc 0.45050878815911194 f1_macro 0.4340926304476996 f1_micro 0.450508788159112\n",
    "'''\n",
    "f1_dict = OrderedDict([('103', 0.3057503506311361),\n",
    " ('107', 0.4759106933019976),\n",
    " ('206', 0.3522267206477733),\n",
    " ('307', 0.2709030100334448),\n",
    " ('402', 0.1748878923766816),\n",
    " ('404', 0.46170212765957447),\n",
    " ('BG', 0.6892655367231638),\n",
    " ('CH', 0.1276595744680851),\n",
    " ('CZ', 0.5058365758754865),\n",
    " ('FR', 0.38764044943820225),\n",
    " ('GE', 0.5833333333333334),\n",
    " ('N', 0.690295358649789),\n",
    " ('PO', 0.3380281690140845),\n",
    " ('TH', 0.6913580246913581),\n",
    " ('TU', 0.45659163987138257)])\n",
    "np.std(list(f1_dict.values())), np.mean(list(f1_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "honey-shoulder",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Non-parametric computation of entropy and mutual-information\n",
    "Adapted by G Varoquaux for code created by R Brette, itself\n",
    "from several papers (see in the code).\n",
    "This code is maintained at https://github.com/mutualinfo/mutual_info \n",
    "Please download the latest code there, to have improvements and \n",
    "bug fixes.\n",
    "These computations rely on nearest-neighbor statistics\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "from scipy.special import gamma,psi\n",
    "from scipy import ndimage\n",
    "from scipy.linalg import det\n",
    "from numpy import pi\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "__all__=['entropy', 'mutual_information', 'entropy_gaussian']\n",
    "\n",
    "EPS = np.finfo(float).eps\n",
    "\n",
    "\n",
    "def nearest_distances(X, k=1):\n",
    "    '''\n",
    "    X = array(N,M)\n",
    "    N = number of points\n",
    "    M = number of dimensions\n",
    "    returns the distance to the kth nearest neighbor for every point in X\n",
    "    '''\n",
    "    knn = NearestNeighbors(n_neighbors=k + 1)\n",
    "    knn.fit(X)\n",
    "    d, _ = knn.kneighbors(X) # the first nearest neighbor is itself\n",
    "    return d[:, -1] # returns the distance to the kth nearest neighbor\n",
    "\n",
    "\n",
    "def entropy_gaussian(C):\n",
    "    '''\n",
    "    Entropy of a gaussian variable with covariance matrix C\n",
    "    '''\n",
    "    if np.isscalar(C): # C is the variance\n",
    "        return .5*(1 + np.log(2*pi)) + .5*np.log(C)\n",
    "    else:\n",
    "        n = C.shape[0] # dimension\n",
    "        return .5*n*(1 + np.log(2*pi)) + .5*np.log(abs(det(C)))\n",
    "\n",
    "\n",
    "def entropy(X, k=1):\n",
    "    ''' Returns the entropy of the X.\n",
    "    Parameters\n",
    "    ===========\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        The data the entropy of which is computed\n",
    "    k : int, optional\n",
    "        number of nearest neighbors for density estimation\n",
    "    Notes\n",
    "    ======\n",
    "    Kozachenko, L. F. & Leonenko, N. N. 1987 Sample estimate of entropy\n",
    "    of a random vector. Probl. Inf. Transm. 23, 95-101.\n",
    "    See also: Evans, D. 2008 A computationally efficient estimator for\n",
    "    mutual information, Proc. R. Soc. A 464 (2093), 1203-1215.\n",
    "    and:\n",
    "    Kraskov A, Stogbauer H, Grassberger P. (2004). Estimating mutual\n",
    "    information. Phys Rev E 69(6 Pt 2):066138.\n",
    "    '''\n",
    "\n",
    "    # Distance to kth nearest neighbor\n",
    "    r = nearest_distances(X, k) # squared distances\n",
    "    n, d = X.shape\n",
    "    volume_unit_ball = (pi**(.5*d)) / gamma(.5*d + 1)\n",
    "    '''\n",
    "    F. Perez-Cruz, (2008). Estimation of Information Theoretic Measures\n",
    "    for Continuous Random Variables. Advances in Neural Information\n",
    "    Processing Systems 21 (NIPS). Vancouver (Canada), December.\n",
    "    return d*mean(log(r))+log(volume_unit_ball)+log(n-1)-log(k)\n",
    "    '''\n",
    "    return (d*np.mean(np.log(r + np.finfo(X.dtype).eps))\n",
    "            + np.log(volume_unit_ball) + psi(n) - psi(k))\n",
    "\n",
    "\n",
    "def mutual_information(variables, k=1):\n",
    "    '''\n",
    "    Returns the mutual information between any number of variables.\n",
    "    Each variable is a matrix X = array(n_samples, n_features)\n",
    "    where\n",
    "      n = number of samples\n",
    "      dx,dy = number of dimensions\n",
    "    Optionally, the following keyword argument can be specified:\n",
    "      k = number of nearest neighbors for density estimation\n",
    "    Example: mutual_information((X, Y)), mutual_information((X, Y, Z), k=5)\n",
    "    '''\n",
    "    if len(variables) < 2:\n",
    "        raise AttributeError(\n",
    "                \"Mutual information must involve at least 2 variables\")\n",
    "    all_vars = np.hstack(variables)\n",
    "    return (sum([entropy(X, k=k) for X in variables])\n",
    "            - entropy(all_vars, k=k))\n",
    "\n",
    "\n",
    "def mutual_information_2d(x, y, sigma=1, normalized=False):\n",
    "    \"\"\"\n",
    "    Computes (normalized) mutual information between two 1D variate from a\n",
    "    joint histogram.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 1D array\n",
    "        first variable\n",
    "    y : 1D array\n",
    "        second variable\n",
    "    sigma: float\n",
    "        sigma for Gaussian smoothing of the joint histogram\n",
    "    Returns\n",
    "    -------\n",
    "    nmi: float\n",
    "        the computed similariy measure\n",
    "    \"\"\"\n",
    "    bins = (256, 256)\n",
    "\n",
    "    jh = np.histogram2d(x, y, bins=bins)[0]\n",
    "\n",
    "    # smooth the jh with a gaussian filter of given sigma\n",
    "    ndimage.gaussian_filter(jh, sigma=sigma, mode='constant',\n",
    "                                 output=jh)\n",
    "\n",
    "    # compute marginal histograms\n",
    "    jh = jh + EPS\n",
    "    sh = np.sum(jh)\n",
    "    jh = jh / sh\n",
    "    s1 = np.sum(jh, axis=0).reshape((-1, jh.shape[0]))\n",
    "    s2 = np.sum(jh, axis=1).reshape((jh.shape[1], -1))\n",
    "\n",
    "    # Normalised Mutual Information of:\n",
    "    # Studholme,  jhill & jhawkes (1998).\n",
    "    # \"A normalized entropy measure of 3-D medical image alignment\".\n",
    "    # in Proc. Medical Imaging 1998, vol. 3338, San Diego, CA, pp. 132-143.\n",
    "    if normalized:\n",
    "        mi = ((np.sum(s1 * np.log(s1)) + np.sum(s2 * np.log(s2)))\n",
    "                / np.sum(jh * np.log(jh))) - 1\n",
    "    else:\n",
    "        mi = ( np.sum(jh * np.log(jh)) - np.sum(s1 * np.log(s1))\n",
    "               - np.sum(s2 * np.log(s2)))\n",
    "\n",
    "    return mi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "offshore-qatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "from espnet.asr.pytorch_backend.asr import load_trained_model\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "if 'cd' not in globals():\n",
    "    cd = True\n",
    "    os.chdir('..')\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "phantom-communist",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:reading model parameters from exp/train_pytorch_wav2vecfexlemblinear/results/snapshot.ep.30\n",
      "WARNING:root:idim 512\n",
      "WARNING:root:fix feature extractor\n",
      "WARNING:root:LangEmb All langs ['101', '103', '107', '203', '206', '307', '402', '404', 'BG', 'CH', 'CR', 'CZ', 'FR', 'GE', 'N', 'PL', 'PO', 'SP', 'TH', 'TU']\n",
      "WARNING:root:Use all features\n",
      "WARNING:root:load g2v npy directly\n",
      "WARNING:root:lang_indices  [1935, 8033, 511, 12196, 13877, 6815, 1340, 657, 5768, 3605, 11307, 5749, 15682, 12835, 9753, 3743, 15536, 14894, 12194, 2867]\n",
      "WARNING:root:n2v_embedding  (20, 256)\n",
      "WARNING:root:lang embedding size torch.Size([20, 425])\n",
      "WARNING:root:warpctc_length_average False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "i = 30\n",
    "model, train_args = load_trained_model(f'exp/train_pytorch_wav2vecfexlemblinear/results/snapshot.ep.{i}')\n",
    "device = torch.device('cuda:1')\n",
    "model = model.float()\n",
    "model = model.to(device)\n",
    "\n",
    "with open('dump/train/deltafalse/data.json.npy') as f:\n",
    "    js = json.load(f)['utts']\n",
    "    \n",
    "def get_lang(d):\n",
    "    s = d.split('_')[0]\n",
    "    s = re.sub(r'\\d+$', '', s.split('-')[0]) if re.search('[a-zA-Z]+', s) else s\n",
    "    return s\n",
    "\n",
    "def to_onehot(ys):\n",
    "    uniq = sorted(list(np.unique(ys)))\n",
    "    \n",
    "    out = np.zeros((len(ys), len(uniq)))\n",
    "    l2int = {l: i for i, l in enumerate(uniq)}\n",
    "    print(l2int)\n",
    "    for i, l in enumerate(ys):\n",
    "        out[i, l2int[l]] = 1\n",
    "    return out\n",
    "\n",
    "def to_int(ys):\n",
    "    uniq = sorted(list(np.unique(ys)))\n",
    "    \n",
    "#     out = np.zeros((len(ys), len(uniq)))\n",
    "    l2int = {l: i for i, l in enumerate(uniq)}\n",
    "    print(l2int)\n",
    "    out = np.array([l2int[l] for i, l in enumerate(ys)])\n",
    "    return out\n",
    "n_sample = 1000\n",
    "frame_ratio = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "satellite-korean",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3998cf8ffcdf4611ad016478215963cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'103': 0, '107': 1, '206': 2, '307': 3, '402': 4, '404': 5, 'BG': 6, 'CH': 7, 'CZ': 8, 'FR': 9, 'GE': 10, 'N': 11, 'PO': 12, 'TH': 13, 'TU': 14}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xs = []\n",
    "ys = []\n",
    "random.seed(2)\n",
    "with torch.no_grad():\n",
    "    for k, v in tqdm(random.sample(js.items(), n_sample)):\n",
    "        lang = get_lang(k)\n",
    "        x = torch.FloatTensor(np.load(v['input'][0]['feat'])).to(device)\n",
    "        # features = model.encode(x, lang_labels=['N'])\n",
    "        features = model.feature_extractor(x.unsqueeze(0))\n",
    "        features = features.transpose(1, 2)\n",
    "        features = model.feature_mapping(features)\n",
    "        \n",
    "        features = features.squeeze(0).detach().cpu().numpy()\n",
    "        T, d = features.shape\n",
    "        n_frame = int(frame_ratio * T)\n",
    "        ys.extend([lang] * n_frame)\n",
    "        idx = random.sample(list(range(T)), n_frame)\n",
    "        xs.append(features[idx])\n",
    "#         break\n",
    "    xs = np.vstack(xs)\n",
    "X = np.array(xs)\n",
    "Y = to_int(ys) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "coral-investor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 512)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "idx = np.random.choice(list(range(len(X))), size=1000, replace=False)\n",
    "X = X[idx]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "outside-founder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 512)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### create data ###\n",
    "sample_count, n = X.shape\n",
    "data = X\n",
    "data_norm = np.sqrt(np.sum(data*data, axis=1))\n",
    "data = data/data_norm[:, None]   # Normalized data to be on unit sphere\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "banner-cloud",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.198765336660309e+156"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## estimate pdf using KDE with gaussian kernel\n",
    "# kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(data)\n",
    "\n",
    "# log_p = kde.score_samples(data)  # returns log(p) of data sample\n",
    "# p = np.exp(log_p)                # estimate p of data sample\n",
    "# entropy = -np.sum(p*log_p)       # evaluate entropy\n",
    "# entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "expensive-magnet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1558339.6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# point wise distance lembliner: w2v + linear\n",
    "np.sum(euclidean_distances(X, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "intense-decimal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1196159.5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# point wise distance lembliner: w2v + linear\n",
    "np.sum(euclidean_distances(data, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-suicide",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "parallel-ceiling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "447365.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# point wise distance lembliner: w2v\n",
    "np.sum(euclidean_distances(X, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "quantitative-beach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1369082.2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# point wise distance lembliner: w2v\n",
    "np.sum(euclidean_distances(data, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-council",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "structural-realtor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(path, linear=False):\n",
    "    model, train_args = load_trained_model(path)\n",
    "    device = torch.device('cuda:1')\n",
    "    model = model.float()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    xs = []\n",
    "    ys = []\n",
    "    random.seed(2)\n",
    "    with torch.no_grad():\n",
    "        for k, v in tqdm(random.sample(js.items(), n_sample)):\n",
    "            lang = get_lang(k)\n",
    "            x = torch.FloatTensor(np.load(v['input'][0]['feat'])).to(device)\n",
    "            # features = model.encode(x, lang_labels=['N'])\n",
    "            features = model.feature_extractor(x.unsqueeze(0))\n",
    "            features = features.transpose(1, 2)\n",
    "            if linear:\n",
    "                features = model.feature_mapping(features)\n",
    "\n",
    "            features = features.squeeze(0).detach().cpu().numpy()\n",
    "            T, d = features.shape\n",
    "            n_frame = int(frame_ratio * T)\n",
    "            ys.extend([lang] * n_frame)\n",
    "            idx = random.sample(list(range(T)), n_frame)\n",
    "            xs.append(features[idx])\n",
    "    #         break\n",
    "        xs = np.vstack(xs)\n",
    "    X = np.array(xs)\n",
    "    Y = to_int(ys)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "welcome-times",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:reading model parameters from exp/train_pytorch_wav2vecfexlemblinear/results/snapshot.ep.30\n",
      "WARNING:root:idim 512\n",
      "WARNING:root:fix feature extractor\n",
      "WARNING:root:LangEmb All langs ['101', '103', '107', '203', '206', '307', '402', '404', 'BG', 'CH', 'CR', 'CZ', 'FR', 'GE', 'N', 'PL', 'PO', 'SP', 'TH', 'TU']\n",
      "WARNING:root:Use all features\n",
      "WARNING:root:load g2v npy directly\n",
      "WARNING:root:lang_indices  [1935, 8033, 511, 12196, 13877, 6815, 1340, 657, 5768, 3605, 11307, 5749, 15682, 12835, 9753, 3743, 15536, 14894, 12194, 2867]\n",
      "WARNING:root:n2v_embedding  (20, 256)\n",
      "WARNING:root:lang embedding size torch.Size([20, 425])\n",
      "WARNING:root:warpctc_length_average False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82dec5c6997d434a93db68dfc276ee6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'103': 0, '107': 1, '206': 2, '307': 3, '402': 4, '404': 5, 'BG': 6, 'CH': 7, 'CZ': 8, 'FR': 9, 'GE': 10, 'N': 11, 'PO': 12, 'TH': 13, 'TU': 14}\n",
      "447365.0 1369082.2\n"
     ]
    }
   ],
   "source": [
    "i = 30\n",
    "path = f'exp/train_pytorch_wav2vecfexlemblinear/results/snapshot.ep.{i}'\n",
    "X, Y = get_samples(path, linear=False)\n",
    "np.random.seed(2)\n",
    "idx = np.random.choice(list(range(len(X))), size=1000, replace=False)\n",
    "X = X[idx]\n",
    "\n",
    "sample_count, n = X.shape\n",
    "data = X\n",
    "data_norm = np.sqrt(np.sum(data*data, axis=1))\n",
    "data = data/data_norm[:, None]   # Normalized data to be on unit sphere\n",
    "\n",
    "print(np.sum(euclidean_distances(X, X)), np.sum(euclidean_distances(data, data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "center-sarah",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:reading model parameters from exp/train_pytorch_wav2vecfexlemblinear/results/snapshot.ep.30\n",
      "WARNING:root:idim 512\n",
      "WARNING:root:fix feature extractor\n",
      "WARNING:root:LangEmb All langs ['101', '103', '107', '203', '206', '307', '402', '404', 'BG', 'CH', 'CR', 'CZ', 'FR', 'GE', 'N', 'PL', 'PO', 'SP', 'TH', 'TU']\n",
      "WARNING:root:Use all features\n",
      "WARNING:root:load g2v npy directly\n",
      "WARNING:root:lang_indices  [1935, 8033, 511, 12196, 13877, 6815, 1340, 657, 5768, 3605, 11307, 5749, 15682, 12835, 9753, 3743, 15536, 14894, 12194, 2867]\n",
      "WARNING:root:n2v_embedding  (20, 256)\n",
      "WARNING:root:lang embedding size torch.Size([20, 425])\n",
      "WARNING:root:warpctc_length_average False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab35bf9471040d49339133eba80d0ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'103': 0, '107': 1, '206': 2, '307': 3, '402': 4, '404': 5, 'BG': 6, 'CH': 7, 'CZ': 8, 'FR': 9, 'GE': 10, 'N': 11, 'PO': 12, 'TH': 13, 'TU': 14}\n",
      "1558339.6 1196159.5\n"
     ]
    }
   ],
   "source": [
    "i = 30\n",
    "path = f'exp/train_pytorch_wav2vecfexlemblinear/results/snapshot.ep.{i}'\n",
    "X, Y = get_samples(path, linear=True)\n",
    "np.random.seed(2)\n",
    "idx = np.random.choice(list(range(len(X))), size=1000, replace=False)\n",
    "X = X[idx]\n",
    "\n",
    "sample_count, n = X.shape\n",
    "data = X\n",
    "data_norm = np.sqrt(np.sum(data*data, axis=1))\n",
    "data = data/data_norm[:, None]   # Normalized data to be on unit sphere\n",
    "\n",
    "print(np.sum(euclidean_distances(X, X)), np.sum(euclidean_distances(data, data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v\n",
    "447365.0 normalized 1369082.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-airplane",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v + linear\n",
    "1558339.6 normalized 1196159.5 entropy - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v + adverserial linear\n",
    "1795634.6 normalized 1375990.0 entropy +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-helicopter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "advanced-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dominican-point",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.45050878815911194 f1_macro 0.4340926304476996 f1_micro 0.450508788159112\n",
      "[('103', 0.3057503506311361),\n",
      " ('107', 0.4759106933019976),\n",
      " ('206', 0.3522267206477733),\n",
      " ('307', 0.2709030100334448),\n",
      " ('402', 0.1748878923766816),\n",
      " ('404', 0.46170212765957447),\n",
      " ('BG', 0.6892655367231638),\n",
      " ('CH', 0.1276595744680851),\n",
      " ('CZ', 0.5058365758754865),\n",
      " ('FR', 0.38764044943820225),\n",
      " ('GE', 0.5833333333333334),\n",
      " ('N', 0.690295358649789),\n",
      " ('PO', 0.3380281690140845),\n",
      " ('TH', 0.6913580246913581),\n",
      " ('TU', 0.45659163987138257)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "\n",
    "clf = LinearSVC(random_state=0).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "from pprint import pprint\n",
    "uniq = sorted(list(np.unique(ys)))\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "f1 = f1_score(y_test, y_pred, average=None)\n",
    "\n",
    "print('acc', acc, 'f1_macro', f1_macro, 'f1_micro', f1_micro)\n",
    "pprint(list(zip(uniq, f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "employed-spotlight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.05399172289086165, 0.23236119058668478, 0.4375800906788145)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accs lemblinear: w2v + linear\n",
    "accs = []\n",
    "for l in uniq:\n",
    "    i = l2int[l]\n",
    "    acc = np.sum((y_pred==i) & (y_test==i)) / np.sum(y_test==i)\n",
    "    accs.append(acc)\n",
    "np.var(accs), np.std(accs), np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "addressed-salon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.05512916208483968, 0.23479600099839792, 0.4305877113086551)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accs lemblinear: w2v\n",
    "accs = []\n",
    "for l in uniq:\n",
    "    i = l2int[l]\n",
    "    acc = np.sum((y_pred==i) & (y_test==i)) / np.sum(y_test==i)\n",
    "    accs.append(acc)\n",
    "np.var(accs), np.std(accs), np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "exposed-southwest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13,  8,  1, ..., 10,  1,  1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "thorough-theology",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2int = {l: i for i, l in enumerate(uniq)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "accessory-engine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'103': 0,\n",
       " '107': 1,\n",
       " '206': 2,\n",
       " '307': 3,\n",
       " '402': 4,\n",
       " '404': 5,\n",
       " 'BG': 6,\n",
       " 'CH': 7,\n",
       " 'CZ': 8,\n",
       " 'FR': 9,\n",
       " 'GE': 10,\n",
       " 'N': 11,\n",
       " 'PO': 12,\n",
       " 'TH': 13,\n",
       " 'TU': 14}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-convertible",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
